{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ActorCritic.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPhfIeo20slvK7WORzWtV3L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzzeeshannn/Robotics/blob/main/ActorCritic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guZk91u0e2Gj",
        "outputId": "33b4ade9-f433-48e4-c829-5e036c3ab8e5"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "!pip install gym\n",
        "!pip install gym[atari]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:11 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [60.9 kB]\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,152 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,413 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [423 kB]\n",
            "Ign:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [798 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,770 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,584 kB]\n",
            "Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [905 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,184 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [452 kB]\n",
            "Get:26 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [41.5 kB]\n",
            "Fetched 13.1 MB in 4s (3,635 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 86 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 1s (656 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 160706 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 86 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.9 [784 kB]\n",
            "Fetched 784 kB in 1s (962 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 163061 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/19/88/7a198a5ee3baa3d547f5a49574cd8c3913b216f5276b690b028f89ffb325/PyVirtualDisplay-2.1-py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.1\n",
            "Collecting piglet\n",
            "  Downloading https://files.pythonhosted.org/packages/11/56/6840e5f45626dc7eb7cd5dff57d11880b3113723b3b7b1fb1fa537855b75/piglet-1.0.0-py2.py3-none-any.whl\n",
            "Collecting piglet-templates\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/cc/bad8acca362189a05a58523f376d24a0a2578a0e386e34e969310ced3100/piglet_templates-1.2.0-py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (2.4.7)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (2.0.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (21.2.0)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (0.36.2)\n",
            "Installing collected packages: piglet-templates, piglet\n",
            "Successfully installed piglet-1.0.0 piglet-templates-1.2.0\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.2.9)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (7.1.2)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ID97uGDe-mM",
        "outputId": "a045b748-c7e7-470e-91e5-213324303ea7"
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/6d/60b97ffc579db665bdd87f2cb47fe1215ae770fbbc1add84ebf36ddca63b/pybullet-3.1.7.tar.gz (79.0MB)\n",
            "\u001b[K     |████████████████████████████████| 79.0MB 60kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pybullet\n",
            "  Building wheel for pybullet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pybullet: filename=pybullet-3.1.7-cp37-cp37m-linux_x86_64.whl size=89750821 sha256=59268ec88174e51dadf40006087c7d27da6d0feaf1aa7b19e9e9b5322b97d30a\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/56/e6/fce8276a2f30165f7ac31089bb72f390fa16b87328651e1a5a\n",
            "Successfully built pybullet\n",
            "Installing collected packages: pybullet\n",
            "Successfully installed pybullet-3.1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoJrzkxrfB6p",
        "outputId": "2360c4a5-720c-4017-ca60-89771256b343"
      },
      "source": [
        "!git clone https://github.com/benelot/pybullet-gym.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pybullet-gym'...\n",
            "remote: Enumerating objects: 804, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 804 (delta 21), reused 18 (delta 5), pack-reused 750\u001b[K\n",
            "Receiving objects: 100% (804/804), 19.31 MiB | 23.10 MiB/s, done.\n",
            "Resolving deltas: 100% (437/437), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JfR6721fGiF",
        "outputId": "ddf4a69f-bfba-4911-8f42-b9de21c6d3ac"
      },
      "source": [
        "cd /content/pybullet-gym/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pybullet-gym\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWHSFDh4fJdl",
        "outputId": "f9181414-d446-4a82-fa01-bb4620563944"
      },
      "source": [
        "!pip install -e ."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/pybullet-gym\n",
            "Requirement already satisfied: pybullet>=1.7.8 in /usr/local/lib/python3.7/dist-packages (from pybulletgym==0.1) (3.1.7)\n",
            "Installing collected packages: pybulletgym\n",
            "  Running setup.py develop for pybulletgym\n",
            "Successfully installed pybulletgym\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fPagPLq3tFD"
      },
      "source": [
        "# Imports here\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) # error only\n",
        "import pybulletgym  # register PyBullet enviroments with open ai gym\n",
        "import pybullet\n",
        "import pybullet_data\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import time\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from collections import deque\n",
        "import shutil\n",
        "\n",
        "# Colab comes with PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from __future__ import division\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from __future__ import division\n",
        "import os\n",
        "import psutil\n",
        "import gc\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNN4cLakF92Q"
      },
      "source": [
        "# A plot function to continously print \n",
        "def subplot(R):\n",
        "    r = R\n",
        "    clear_output(wait=True)\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(4,4))\n",
        "    ax.plot(list(r[1]), list(r[0]), 'r') #row=0, col=0\n",
        "    plt.show()"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLKa2wVu3yah"
      },
      "source": [
        "# Defining the memory buffer here\n",
        "class Memory: \n",
        "  def __init__(self, size): \n",
        "    self.buffer = deque(maxlen = size)        # Deque is a list-like collection which allows for fast-appends and pops on either side\n",
        "    self.len = 0                              # Initialize this to 0, we keep increasing it as and when we add our experience to the Memory\n",
        "    self.size = size                          # The maximum possible size of the Memory buffer\n",
        "  \n",
        "  def add(self, state, action, reward, next_state): \n",
        "    \"\"\"\n",
        "    This function is used to add the experience to the memory buffer\n",
        "    We will be saving this as a tuple which makes it simpler to associate pairs while sampling\n",
        "    \"\"\"\n",
        "    record = (state, action, reward, next_state)\n",
        "    # Add the record to the memory\n",
        "    self.len += 1\n",
        "    if self.len > self.size:\n",
        "      self.len = self.size\n",
        "    self.buffer.append(record)\n",
        "  \n",
        "  def sample(self, sample_size): \n",
        "    \"\"\"\n",
        "    This function is used to sample experiences from the memory\n",
        "    Input Parameter: sample_size decides the size of sample\n",
        "    Returns: A batch of states, actions, rewards and next states in order\n",
        "    \"\"\"\n",
        "    sampled_experience = []\n",
        "    \n",
        "    if sample_size < self.size: \n",
        "      sample_size = self.len\n",
        "    \n",
        "    sampled_experience = random.sample(self.buffer, sample_size)\n",
        "    \n",
        "    state_experience = []\n",
        "    action_experience = []\n",
        "    reward_experience = []\n",
        "    next_state_experience = []\n",
        "\n",
        "    # Important to convert it to ndarray \n",
        "    for every_experience in sampled_experience: \n",
        "      state_experience.append(np.float32(every_experience[0]))\n",
        "      action_experience.append(np.float32(every_experience[1]))\n",
        "      reward_experience.append(np.float32(every_experience[2]))\n",
        "      next_state_experience.append(np.float32(every_experience[3]))\n",
        "    \n",
        "    return state_experience, action_experience, reward_experience, next_state_experience"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj-fbiWTVymI"
      },
      "source": [
        "EPS = 0.003\n",
        "\n",
        "def fanin_init(size, fanin=None):\n",
        "\tfanin = fanin or size[0]\n",
        "\tv = 1. / np.sqrt(fanin)\n",
        "\treturn torch.Tensor(size).uniform_(-v, v)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBraI62Z81nw"
      },
      "source": [
        "# Defining the Actor Network \n",
        "class Actor(nn.Module): \n",
        "  def __init__(self, state_dimensions, action_dimensions): \n",
        "    \"\"\"\n",
        "    Input Parameters: \n",
        "    Since the input to the network will be your state, the state dimensions are required\n",
        "    Each node of the final layer of the network will correspond to one of the actions, hence the final \n",
        "    output layers should be the same as the number of actions\n",
        "    \"\"\"\n",
        "    super(Actor, self).__init__()\n",
        "    \n",
        "    self.state_dim = state_dimensions\n",
        "    self.action_dim = action_dimensions\n",
        "\n",
        "    self.fc1 = nn.Linear(self.state_dim, 64)\n",
        "    self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
        "    self.fc2 = nn.Linear(64,128)\n",
        "    self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
        "    self.fc3 = nn.Linear(128, 256)\n",
        "    self.fc3.weight.data = fanin_init(self.fc3.weight.data.size())\n",
        "    self.fc4 = nn.Linear(256, self.action_dim)\n",
        "    self.fc4.weight.data.uniform_(- EPS, EPS)\n",
        "  \n",
        "  def forward(self, state): \n",
        "    output = self.fc1(state)\n",
        "    output = F.relu(output)\n",
        "    output = self.fc2(output)\n",
        "    output = F.relu(output)\n",
        "    output = self.fc3(output)\n",
        "    output = F.relu(output)\n",
        "    action = self.fc4(output)\n",
        "\n",
        "    return action"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA-pzLzaA8Lf"
      },
      "source": [
        "# Defining the Critic Network\n",
        "class Critic(nn.Module): \n",
        "  def __init__(self, state_dimensions, action_dimensions):\n",
        "    super(Critic, self).__init__()\n",
        "\n",
        "    self.state_dim = state_dimensions\n",
        "    self.action_dim = action_dimensions\n",
        "\n",
        "    self.fcs1 = nn.Linear(self.state_dim, 64)\n",
        "    self.fcs1.weight.data = fanin_init(self.fcs1.weight.data.size())\n",
        "    self.fcs2 = nn.Linear(64, 128)\n",
        "    self.fcs2.weight.data = fanin_init(self.fcs2.weight.data.size())\n",
        "\n",
        "    self.fca1 = nn.Linear(self.action_dim, 64)\n",
        "    self.fca1.weight.data = fanin_init(self.fca1.weight.data.size())\n",
        "    self.fca2 = nn.Linear(64, 128)\n",
        "    self.fca2.weight.data = fanin_init(self.fca2.weight.data.size())\n",
        "\n",
        "    self.fc1 = nn.Linear(256, 512)\n",
        "    self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
        "    self.fc2 = nn.Linear(512, 1)\n",
        "    self.fc2.weight.data.uniform_(- EPS, EPS)\n",
        "  \n",
        "  def forward(self, state, action):\n",
        "    state1 = self.fcs1(state)\n",
        "    state1 = F.relu(state1)\n",
        "    state2 = self.fcs2(state1)\n",
        "    state2 = F.relu(state2)\n",
        "\n",
        "    action1 = self.fca1(action)\n",
        "    action1 = F.relu(action1)\n",
        "    action2 = self.fca2(action1)\n",
        "    action2 = F.relu(action2)\n",
        "\n",
        "    temp = torch.cat((state2, action2), dim=1)\n",
        "\n",
        "    output = self.fc1(temp)\n",
        "    output = F.relu(output)\n",
        "    output = self.fc2(output)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-l9PclohxTV"
      },
      "source": [
        "# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
        "class OrnsteinUhlenbeckActionNoise:\n",
        "\n",
        "\tdef __init__(self, action_dim, mu = 0, theta = 0.15, sigma = 0.2):\n",
        "\t\tself.action_dim = action_dim\n",
        "\t\tself.mu = mu\n",
        "\t\tself.theta = theta\n",
        "\t\tself.sigma = sigma\n",
        "\t\tself.X = np.ones(self.action_dim) * self.mu\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\tself.X = np.ones(self.action_dim) * self.mu\n",
        "\n",
        "\tdef sample(self):\n",
        "\t\tdx = self.theta * (self.mu - self.X)\n",
        "\t\tdx = dx + self.sigma * np.random.randn(len(self.X))\n",
        "\t\tself.X = self.X + dx\n",
        "\t\treturn self.X"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4f0dAnIjMs7"
      },
      "source": [
        "def soft_update(target, source, tau):\n",
        "  for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "    target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hts3bmWjOv5"
      },
      "source": [
        "def hard_update(target, source):\n",
        "  for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "    target_param.data.copy_(param.data)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpUzeeDWik2s"
      },
      "source": [
        "# Defining a few parameters here\n",
        "tau = 0.009\n",
        "lr = 0.001\n",
        "batch_size = 128\n",
        "gamma = 0.99\n",
        "MAX_EPISODES = 20000\n",
        "MAX_STEPS = 1000\n",
        "MAX_BUFFER = 1000000\n",
        "MAX_TOTAL_REWARD = 300\n",
        "PRINT_EVERY = 2\n",
        "plot_reward = []"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-MypVJhg3FY"
      },
      "source": [
        "# We define our training module here\n",
        "class Trainer: \n",
        "  def __init__(self, state_dimensions, action_dimensions, memory): \n",
        "    self.state_dim = state_dimensions\n",
        "    self.action_dim = action_dimensions\n",
        "    self.memory = memory\n",
        "    self.iter_counter = 0\n",
        "    self.noise = OrnsteinUhlenbeckActionNoise(self.action_dim)\n",
        "\n",
        "    self.actor = Actor(self.state_dim, self.action_dim)\n",
        "    self.target_actor = Actor(self.state_dim, self.action_dim)\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr)\n",
        "\n",
        "    self.critic = Critic(self.state_dim, self.action_dim)\n",
        "    self.target_critic = Critic(self.state_dim, self.action_dim)\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr)\n",
        "\n",
        "    # Update here\n",
        "    # You can either soft update or hard update between your target and current network\n",
        "    # Hard update directly copies the parameters from source to target network \n",
        "    # Soft update: y = tau*x + (1-tau)*y\n",
        "\n",
        "    hard_update(self.target_actor, self.actor)\n",
        "    hard_update(self.target_critic, self.critic)\n",
        "  \n",
        "  def get_action_with_noise(self, state): \n",
        "    state = Variable(torch.from_numpy(state))\n",
        "    action = self.target_actor.forward(state).detach()\n",
        "    action_with_noise = action.data.numpy() \n",
        "\n",
        "    return action_with_noise\n",
        "  \n",
        "  def optimize(self): \n",
        "    \"\"\"\n",
        "    A function to optimize both actor-critic networks by sampling from experience buffer\n",
        "    \"\"\"\n",
        "\n",
        "    state_experiences, action_experiences, rewards_experiences, next_state_batch = self.memory.sample(batch_size)\n",
        "    \n",
        "    # Converting it to a nd-array from list\n",
        "    state_experiences = np.array(state_experiences)\n",
        "    action_experiences = np.array(action_experiences)\n",
        "    rewards_experiences = np.array(rewards_experiences)\n",
        "    next_state_batch = np.array(next_state_batch)\n",
        "\n",
        "    state_experiences = Variable(torch.from_numpy(state_experiences))\n",
        "    action_experiences = Variable(torch.from_numpy(action_experiences))\n",
        "    rewards_experiences = Variable(torch.from_numpy(rewards_experiences))\n",
        "    next_state_batch = Variable(torch.from_numpy(next_state_batch))\n",
        "\n",
        "    # Critic Network Optimization\n",
        "    # Generating a new action for the next state sampled from the experience\n",
        "    new_action = self.target_actor.forward(next_state_batch).detach()   \n",
        "    # Generating the state-action value for this new state-action pair\n",
        "    next_qvalue = torch.squeeze(self.target_critic.forward(next_state_batch, new_action).detach())    \n",
        "    current_qval = torch.squeeze(self.target_critic.forward(state_experiences, action_experiences).detach())\n",
        "    # Calculating the advantage \n",
        "    advantage = rewards_experiences + gamma*next_qvalue - current_qval\n",
        "    # Prediciton \n",
        "    y_predicted = torch.squeeze(self.critic.forward(state_experiences, action_experiences))\n",
        "    # Computing loss and updating the network\n",
        "    critic_loss = F.smooth_l1_loss(y_predicted, advantage)\n",
        "    self.critic_optimizer.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    self.critic_optimizer.step()\n",
        "\n",
        "    # Actor Network Optimization\n",
        "    predicted_action = self.actor.forward(state_experiences)\n",
        "    # Computing loss and updating the network\n",
        "    actor_loss = -1 * torch.sum(self.critic.forward(state_experiences, predicted_action)*advantage)\n",
        "    self.actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    self.actor_optimizer.step()\n",
        "\n",
        "    # Update here \n",
        "    soft_update(self.target_actor, self.actor, tau)\n",
        "    soft_update(self.target_critic, self.critic, tau)\n"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLWLKVr5C2lh"
      },
      "source": [
        "# OpenAI Wrappers \n",
        "# You can directly import them as well\n",
        "class NormalizedEnv(gym.ActionWrapper):\n",
        "    def action(self, action):\n",
        "        act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
        "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
        "        return act_k * action + act_b\n",
        "\n",
        "    def reverse_action(self, action):\n",
        "        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
        "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
        "        return act_k_inv * (action - act_b)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g8wnvijDBpq"
      },
      "source": [
        "# Another wrapper class\n",
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my9qRN0xDNRF"
      },
      "source": [
        "env = NormalizedEnv(gym.make(\"InvertedPendulumMuJoCoEnv-v0\"))\n",
        "env = ClipRewardEnv(env)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmazrkQlDPNS",
        "outputId": "131455ec-16f6-42bf-f542-1cefe2592acd"
      },
      "source": [
        "state_dimensions = env.observation_space.shape[0]\n",
        "action_dimensions = env.action_space.shape[0]\n",
        "\n",
        "print(\"State Space: \", state_dimensions)\n",
        "print(\"Action Space: \", action_dimensions)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State Space:  4\n",
            "Action Space:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQjKbBp2D5Db"
      },
      "source": [
        "# Initiate the memory buffer and trainer\n",
        "memory = Memory(MAX_BUFFER)\n",
        "train = Trainer(state_dimensions, action_dimensions, memory)"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-AwTFx8EK2Q",
        "outputId": "7fbc9889-83fd-4a25-c03e-64e1011650ff"
      },
      "source": [
        "# Training procedure\n",
        "for every_episode in range(MAX_EPISODES): \n",
        "  ep_reward = 0\n",
        "  obs = env.reset()\n",
        "  for t in range(100):\n",
        "    state = np.float32(obs)\n",
        "    action = train.get_action_with_noise(state)\n",
        "    # Pass the action to the state\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "    ep_reward += reward\n",
        "\n",
        "    if done: \n",
        "      new_state = None\n",
        "    else: \n",
        "      new_state = np.float32(new_state)\n",
        "      # Save the experience in the memory buffer\n",
        "      memory.add(state, action, reward, new_state)\n",
        "    \n",
        "    state = new_state\n",
        "    train.optimize()\n",
        "    \n",
        "    # Break the loop if environment has terminated\n",
        "    if done: \n",
        "      break\n",
        "  print(\" Episode number and Reward: \", every_episode, ep_reward)\n",
        "  #plot_reward.append(ep_reward)\n",
        "  #if (every_episode % PRINT_EVERY) == (PRINT_EVERY-1):\n",
        "  #  plt.plot(plot_reward)\n",
        "  #gc.collect()\n",
        "plt.plot(ep_reward)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Episode number and Reward:  0 11.0\n",
            " Episode number and Reward:  1 7.0\n",
            " Episode number and Reward:  2 10.0\n",
            " Episode number and Reward:  3 8.0\n",
            " Episode number and Reward:  4 10.0\n",
            " Episode number and Reward:  5 11.0\n",
            " Episode number and Reward:  6 10.0\n",
            " Episode number and Reward:  7 11.0\n",
            " Episode number and Reward:  8 9.0\n",
            " Episode number and Reward:  9 7.0\n",
            " Episode number and Reward:  10 10.0\n",
            " Episode number and Reward:  11 8.0\n",
            " Episode number and Reward:  12 10.0\n",
            " Episode number and Reward:  13 10.0\n",
            " Episode number and Reward:  14 8.0\n",
            " Episode number and Reward:  15 12.0\n",
            " Episode number and Reward:  16 12.0\n",
            " Episode number and Reward:  17 10.0\n",
            " Episode number and Reward:  18 9.0\n",
            " Episode number and Reward:  19 6.0\n",
            " Episode number and Reward:  20 10.0\n",
            " Episode number and Reward:  21 10.0\n",
            " Episode number and Reward:  22 11.0\n",
            " Episode number and Reward:  23 10.0\n",
            " Episode number and Reward:  24 9.0\n",
            " Episode number and Reward:  25 7.0\n",
            " Episode number and Reward:  26 7.0\n",
            " Episode number and Reward:  27 9.0\n",
            " Episode number and Reward:  28 8.0\n",
            " Episode number and Reward:  29 11.0\n",
            " Episode number and Reward:  30 8.0\n",
            " Episode number and Reward:  31 11.0\n",
            " Episode number and Reward:  32 11.0\n",
            " Episode number and Reward:  33 11.0\n",
            " Episode number and Reward:  34 7.0\n",
            " Episode number and Reward:  35 9.0\n",
            " Episode number and Reward:  36 8.0\n",
            " Episode number and Reward:  37 6.0\n",
            " Episode number and Reward:  38 9.0\n",
            " Episode number and Reward:  39 6.0\n",
            " Episode number and Reward:  40 8.0\n",
            " Episode number and Reward:  41 12.0\n",
            " Episode number and Reward:  42 8.0\n",
            " Episode number and Reward:  43 9.0\n",
            " Episode number and Reward:  44 11.0\n",
            " Episode number and Reward:  45 7.0\n",
            " Episode number and Reward:  46 7.0\n",
            " Episode number and Reward:  47 9.0\n",
            " Episode number and Reward:  48 7.0\n",
            " Episode number and Reward:  49 8.0\n",
            " Episode number and Reward:  50 7.0\n",
            " Episode number and Reward:  51 8.0\n",
            " Episode number and Reward:  52 11.0\n",
            " Episode number and Reward:  53 7.0\n",
            " Episode number and Reward:  54 8.0\n",
            " Episode number and Reward:  55 10.0\n",
            " Episode number and Reward:  56 10.0\n",
            " Episode number and Reward:  57 10.0\n",
            " Episode number and Reward:  58 7.0\n",
            " Episode number and Reward:  59 10.0\n",
            " Episode number and Reward:  60 12.0\n",
            " Episode number and Reward:  61 11.0\n",
            " Episode number and Reward:  62 6.0\n",
            " Episode number and Reward:  63 7.0\n",
            " Episode number and Reward:  64 12.0\n",
            " Episode number and Reward:  65 10.0\n",
            " Episode number and Reward:  66 11.0\n",
            " Episode number and Reward:  67 10.0\n",
            " Episode number and Reward:  68 6.0\n",
            " Episode number and Reward:  69 8.0\n",
            " Episode number and Reward:  70 11.0\n",
            " Episode number and Reward:  71 8.0\n",
            " Episode number and Reward:  72 11.0\n",
            " Episode number and Reward:  73 12.0\n",
            " Episode number and Reward:  74 8.0\n",
            " Episode number and Reward:  75 10.0\n",
            " Episode number and Reward:  76 9.0\n",
            " Episode number and Reward:  77 7.0\n",
            " Episode number and Reward:  78 8.0\n",
            " Episode number and Reward:  79 11.0\n",
            " Episode number and Reward:  80 9.0\n",
            " Episode number and Reward:  81 11.0\n",
            " Episode number and Reward:  82 10.0\n",
            " Episode number and Reward:  83 8.0\n",
            " Episode number and Reward:  84 10.0\n",
            " Episode number and Reward:  85 11.0\n",
            " Episode number and Reward:  86 10.0\n",
            " Episode number and Reward:  87 7.0\n",
            " Episode number and Reward:  88 11.0\n",
            " Episode number and Reward:  89 10.0\n",
            " Episode number and Reward:  90 9.0\n",
            " Episode number and Reward:  91 7.0\n",
            " Episode number and Reward:  92 11.0\n",
            " Episode number and Reward:  93 9.0\n",
            " Episode number and Reward:  94 10.0\n",
            " Episode number and Reward:  95 10.0\n",
            " Episode number and Reward:  96 11.0\n",
            " Episode number and Reward:  97 10.0\n",
            " Episode number and Reward:  98 7.0\n",
            " Episode number and Reward:  99 7.0\n",
            " Episode number and Reward:  100 7.0\n",
            " Episode number and Reward:  101 10.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}